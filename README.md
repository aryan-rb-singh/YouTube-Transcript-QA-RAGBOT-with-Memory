# 🎥 YouTube Transcript Question Answering RAGBOT with Memory using PPLX

This project is an end-to-end pipeline that lets users **ask questions about YouTube videos** using the **video transcript as context**, powered by **LangChain**, **Mistral LLM**, **FAISS**, and **HuggingFace embeddings**. It showcases how to integrate retrieval-based systems with large language models for accurate, grounded answers.

---

## 📌 What This Project Does

This system enables you to:
- 🧠 Extract English transcripts from YouTube videos using their ID
- 🧩 Split long transcripts into smaller, context-preserving chunks
- 🔍 Generate embeddings for these chunks using HuggingFace transformers
- 🗃️ Store them in a FAISS vector database for efficient similarity search
- ❓ Query the transcript by asking natural language questions
- 🤖 Get reliable, grounded answers generated by Mistral's large language model

All questions are answered **only using the transcript** context. If the video doesn't include enough relevant information, the system will say it doesn’t know — ensuring honesty and transparency.

---

## 🛠️ Tech Stack

| Component             | Technology                                       |
|----------------------|--------------------------------------------------|
| Transcript Fetching  | `youtube-transcript-api`                         |
| Text Splitting       | `langchain.text_splitter`                        |
| Embedding Model      | `all-MiniLM-L6-v2` (HuggingFace)                 |
| Vector Database      | FAISS (Facebook AI Similarity Search)           |
| LLM Engine           | `mistral-medium` via `langchain-mistralai`       |
| Pipeline Framework   | LangChain                                        |
| Environment Vars     | `python-dotenv`                                  |
| Programming Language | Python 3.10+                                     |

---

## 🚀 How It Works

### Step 1: Transcript Extraction
The system uses the `youtube-transcript-api` to fetch English subtitles from a given YouTube video ID. If the video has no captions, it gracefully notifies the user.

### Step 2: Chunking Text
Using LangChain's `RecursiveCharacterTextSplitter`, the transcript is broken into overlapping chunks (1000 characters with 200 overlap) to preserve context and avoid cutting off semantic meaning mid-sentence.

### Step 3: Embeddings
Each chunk is passed through a pre-trained transformer model (`all-MiniLM-L6-v2`) from HuggingFace to generate high-dimensional vector embeddings that capture semantic meaning.

### Step 4: Vector Store
The vector representations are stored in FAISS, a high-speed vector search engine. This allows fast retrieval of the most relevant transcript chunks for a given query.

### Step 5: Retrieval + LLM
When a question is asked:
- The top 3 most relevant chunks are retrieved from FAISS
- These chunks are inserted into a prompt template
- The prompt is sent to a Mistral LLM via `ChatMistralAI`
- The LLM responds only using the provided context

---
